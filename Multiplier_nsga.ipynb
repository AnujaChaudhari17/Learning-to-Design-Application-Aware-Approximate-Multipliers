{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZn_939TBeae"
      },
      "outputs": [],
      "source": [
        "# clear any previous failed builds\n",
        "!rm -rf ~/.cache/torch_extensions/*\n",
        "\n",
        "# tell the build where CUDA lives on Colab\n",
        "import os\n",
        "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
        "os.environ[\"CUDA_PATH\"] =  \"/usr/local/cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILk44gwGBmst",
        "outputId": "c7721d26-dd3c-45f0-a2e2-13444837cc76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/etrommer/torch-approx.git\n",
            "  Cloning https://github.com/etrommer/torch-approx.git to /tmp/pip-req-build-d7og320v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/etrommer/torch-approx.git /tmp/pip-req-build-d7og320v\n",
            "  Resolved https://github.com/etrommer/torch-approx.git to commit 34417226bc1f2d2e0fb82cea905b8daa1527be46\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.15.1\n",
            "  Downloading torchvision-0.15.1-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchaudio==2.0.1\n",
            "  Downloading torchaudio-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.1) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.1) (11.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1) (2025.6.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.15.1-cp311-cp311-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.0.1-cp311-cp311-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torchapprox\n",
            "  Building wheel for torchapprox (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchapprox: filename=torchapprox-0.2.0-py3-none-any.whl size=30644 sha256=ca75ce7aa99a974a5ae59b7624577565399f18e1b3f28049fdfe7a2bdefd079b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kvflssw3/wheels/8b/18/77/417bca4492fbb0616df136820f51920eddbfc8f7c806fcb716\n",
            "Successfully built torchapprox\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision, torchaudio, torchapprox\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\n",
            "blosc2 3.5.0 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 numpy-1.25.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchapprox-0.2.0 torchaudio-2.0.1 torchvision-0.15.1 triton-2.0.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ad0dbc04df04401f82fc7e6029e95eb9",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install numpy==1.25.2 \\\n",
        "            torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 \\\n",
        "            git+https://github.com/etrommer/torch-approx.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHK-_fyhBqdY",
        "outputId": "4ddd7ca3-f649-4dd6-efc8-a5b84e224ec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libomp-14-dev libomp5-14\n",
            "Suggested packages:\n",
            "  libomp-14-doc\n",
            "The following NEW packages will be installed:\n",
            "  libomp-14-dev libomp-dev libomp5-14 ninja-build\n",
            "0 upgraded, 4 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 850 kB of archives.\n",
            "After this operation, 9,349 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libomp5-14 amd64 1:14.0.0-1ubuntu1.1 [389 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libomp-14-dev amd64 1:14.0.0-1ubuntu1.1 [347 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ninja-build amd64 1.10.1-1 [111 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libomp-dev amd64 1:14.0-55~exp2 [3,074 B]\n",
            "Fetched 850 kB in 2s (343 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libomp5-14:amd64.\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5-14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libomp5-14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libomp-14-dev.\n",
            "Preparing to unpack .../libomp-14-dev_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libomp-14-dev (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package ninja-build.\n",
            "Preparing to unpack .../ninja-build_1.10.1-1_amd64.deb ...\n",
            "Unpacking ninja-build (1.10.1-1) ...\n",
            "Selecting previously unselected package libomp-dev:amd64.\n",
            "Preparing to unpack .../libomp-dev_1%3a14.0-55~exp2_amd64.deb ...\n",
            "Unpacking libomp-dev:amd64 (1:14.0-55~exp2) ...\n",
            "Setting up libomp5-14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up ninja-build (1.10.1-1) ...\n",
            "Setting up libomp-14-dev (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libomp-dev:amd64 (1:14.0-55~exp2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install libomp-dev ninja-build\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmOXf2rwEitz"
      },
      "outputs": [],
      "source": [
        "# Base code as a string\n",
        "base_code = \"\"\"\n",
        "def AND(A,B):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    return A&B\n",
        "def nand(a, b):\n",
        "    return 0 if (a == 1 and b == 1) else 1\n",
        "def xnor(a,b):\n",
        "    return 1 if a==b else 0\n",
        "def nor(a, b):\n",
        "    return 1 if (a == 0 and b == 0) else 0\n",
        "def half_adder(A, B):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    sum_bit = A ^ B\n",
        "    carry = A & B\n",
        "    return [sum_bit, carry]\n",
        "\n",
        "def full_adder(A,B,C):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    sum = A^B^C\n",
        "    carry = (A&B) | (B&C) | (A&C)\n",
        "    return [sum,carry]\n",
        "\n",
        "# Basic compressors\n",
        "def ahmad(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = nand(nor(A,B),nor(C,D))\n",
        "    carry = nor(nor(A,B),nor(C,D))\n",
        "    return [sum,carry]\n",
        "\n",
        "def akbari(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = nand(xnor(A,B),xnor(C,D))\n",
        "    carry = nand(nand(A,B),nand(C,D))\n",
        "    return [sum,carry]\n",
        "\n",
        "def meo(x1,x2,x3,x4):\n",
        "    num_bi = [x4, x3, x2, x1]\n",
        "    num_de = int(''.join(str(x) for x in num_bi), base=2)\n",
        "\n",
        "    if num_de == 0:\n",
        "        sum_a = 0\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de == 1:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de == 2:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==3:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de==4:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==5:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==6:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==7:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==8:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==9:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==10:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==11:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==12:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==13:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==14:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    else:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "\n",
        "    return [sum_a,carry_a]\n",
        "\n",
        "\n",
        "def venka(x1,x2,x3,x4):\n",
        "    num_bi = [x4, x3, x2, x1]\n",
        "    num_de = int(''.join(str(x) for x in num_bi), base=2)\n",
        "\n",
        "    if num_de == 0:\n",
        "        sum_a = 0\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de == 1:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de == 2:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==3:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de==4:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==5:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==6:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==7:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==8:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==9:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==10:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==11:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==12:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==13:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==14:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "    else:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "\n",
        "    return [sum_a,carry_a]\n",
        "\n",
        "def yang(x1,x2,x3,x4):\n",
        "    num_bi = [x4, x3, x2, x1]\n",
        "    num_de = int(''.join(str(x) for x in num_bi), base=2)\n",
        "\n",
        "    if num_de == 0:\n",
        "        sum_a = 0\n",
        "        carry_a = 0\n",
        "    elif num_de == 1:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "    elif num_de == 2:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "    elif num_de ==3:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "    elif num_de==4:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "    elif num_de ==5:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "    elif num_de ==6:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "    elif num_de ==7:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "    elif num_de ==8:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "    elif num_de ==9:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "    elif num_de ==10:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "    elif num_de ==11:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "    elif num_de ==12:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "    elif num_de ==13:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "    elif num_de ==14:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "    else:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    return [sum_a,carry_a]\n",
        "\n",
        "def momeni(x1,x2,x3,x4):\n",
        "    num_bi = [x4, x3, x2, x1]\n",
        "    num_de = int(''.join(str(x) for x in num_bi), base=2)\n",
        "\n",
        "    if num_de == 0 :\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de == 1:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de == 2:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==3:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de==4:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==5:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==6:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==7:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==8:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==9:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==10:\n",
        "        sum_a = 0\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==11:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==12:\n",
        "        sum_a = 1\n",
        "        carry_a = 0\n",
        "\n",
        "    elif num_de ==13:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    elif num_de ==14:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "    else:\n",
        "        sum_a = 1\n",
        "        carry_a = 1\n",
        "\n",
        "\n",
        "    return [sum_a,carry_a]\n",
        "def mux(sel, data0, data1):\n",
        "    if sel == 0:\n",
        "        return data0\n",
        "    elif sel == 1:\n",
        "        return data1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def app_compressor(A,B,C,D):\n",
        "  a = int(A)\n",
        "  b = int(B)\n",
        "  c = int(C)\n",
        "  d = int(D)\n",
        "  temp1 = a^b\n",
        "  temp2 = nand(a,b)\n",
        "  temp3 = c^d\n",
        "  temp4 = nand(c,d)\n",
        "\n",
        "  temp5 = nand(temp1,temp3)\n",
        "  temp6 = 1^temp1\n",
        "  temp7 = AND(temp2,temp4)\n",
        "\n",
        "  #sum\n",
        "  sum = mux(temp3,temp1,temp6)\n",
        "\n",
        "  #carry\n",
        "  carry = nand(temp7,temp5)\n",
        "\n",
        "  return [sum,carry]\n",
        "\n",
        "# AC6G compressors\n",
        "def AC6G1(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|B)|(C|D))\n",
        "    carry = (A&(C|D)|(B&C))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G2(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|B)|(C|D))\n",
        "    carry = (A&(C|D)|(B&D))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G3(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|C)|(B|D))\n",
        "    carry = (A&(C|D)|(D&C))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G4(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|C)|(B|D))\n",
        "    carry = (B& (C|D)|(A&C))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G5(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|D)|(B|C))\n",
        "    carry = (B&(C|D)|(A&D))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G6(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|D)|(B|C))\n",
        "    carry = (B&(C|D)|(D&C))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G7(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|B)|(C|D))\n",
        "    carry = ((C&(A|B))|(A&B))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G8(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|C)|(B|D))\n",
        "    carry = ((D&(A|B))|(A&B))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G9(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|D)|(B|C))\n",
        "    carry = ((A&(B|D))|(B&C))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G10(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|B)|(C|D))\n",
        "    carry = ((A&(B|D))|(C&D))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G11(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|C)|(B|D))\n",
        "    carry = ((C&(B|D))|(A&B))\n",
        "    return [sum,carry]\n",
        "\n",
        "def AC6G12(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = ((A|D)|(B|C))\n",
        "    carry = ((C&(B|D))|(A&D))\n",
        "    return [sum,carry]\n",
        "\n",
        "# ACFGI compressors\n",
        "def ACFGI1(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = 1\n",
        "    carry = A\n",
        "    return [sum,carry]\n",
        "\n",
        "def ACFGII1(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = A\n",
        "    carry = B\n",
        "    return [sum,carry]\n",
        "\n",
        "def ACFGII10(A,B,C,D):\n",
        "    A=int(A)\n",
        "    B=int(B)\n",
        "    C=int(C)\n",
        "    D=int(D)\n",
        "    sum = D\n",
        "    carry = A\n",
        "    return [sum,carry]\n",
        "\n",
        "###############################MULTIPLIER##################\n",
        "def multiply2(A,B):\n",
        "    A=bin(A)[2:]\n",
        "    B=bin(B)[2:]\n",
        "    A=A.zfill(8)\n",
        "    B=B.zfill(8)\n",
        "    L=[]\n",
        "    N=len(A)\n",
        "    M=len(B)\n",
        "\n",
        "    for i in range(M):\n",
        "        k = []\n",
        "        for j in range(N):\n",
        "            k.append(AND(A[j],B[M-1-i]))\n",
        "        L.append(k)\n",
        "\n",
        "    ##column 5\n",
        "    (x20,y20) = COMPRESSORa(L[0][3],L[1][4],L[2][5],L[3][6])\n",
        "    (x21,y21) = COMPRESSORb(L[4][7],x20,0,0)\n",
        "\n",
        "    ##column 6\n",
        "    (x0,y0)   = COMPRESSORc(L[0][2],L[1][3],L[2][4],L[3][5])\n",
        "    (x19,y19) = COMPRESSORd(L[4][6],L[5][7],x0,y20)\n",
        "    (m5,c5)   = half_adder(x19,y21)\n",
        "\n",
        "    ##column 7\n",
        "    (x1,y1) = COMPRESSORe(L[0][1],L[1][2],L[2][3],L[3][4])\n",
        "    (x2,y2) = COMPRESSORf(L[4][5],L[5][6],L[6][7],0)\n",
        "    (x3,y3) = COMPRESSORg(x1,x2,y0,0)\n",
        "    (m6,c6) = full_adder(x3,y19,c5)\n",
        "\n",
        "    ##column 8\n",
        "    (x4,y4) = COMPRESSORh(L[0][0],L[1][1],L[2][2],L[3][3])\n",
        "    (x5,y5) = COMPRESSORi(L[4][4],L[5][5],L[6][6],L[7][7])\n",
        "    (x6,y6) = COMPRESSORj(x4,x5,y1,y2)\n",
        "    (m7,c7) = full_adder(x6,y3,c6)\n",
        "\n",
        "    ##column 9\n",
        "    (x7,y7) = COMPRESSORk(L[1][0],L[2][1],L[3][2],L[4][3])\n",
        "    (x8,y8) = COMPRESSORl(L[5][4],L[6][5],L[7][6],0)\n",
        "    (x9,y9) = COMPRESSORm(x7,x8,y4,y5)\n",
        "    (m8,c8) = full_adder(x9,y6,c7)\n",
        "\n",
        "    ##column 10\n",
        "    (x10,y10) = COMPRESSORn(L[2][0],L[3][1],L[4][2],L[5][3])\n",
        "    (x11,y11) = COMPRESSORo(L[6][4],L[7][5],0,0)\n",
        "    (x12,y12) = COMPRESSORp(x10,x11,y7,y8)\n",
        "    (m9,c9)   = full_adder(x12,y9,c8)\n",
        "\n",
        "    ##column 11\n",
        "    (x13,y13) = COMPRESSORq(L[3][0],L[4][1],L[5][2],L[6][3])\n",
        "    (x14,y14) = COMPRESSORr(L[7][4],x13,y10,y11)\n",
        "    (m10,c10) = full_adder(x14,y12,c9)\n",
        "\n",
        "    ##column 12\n",
        "    (x15,y15) = COMPRESSORs(L[4][0],L[5][1],L[6][2],L[7][3])\n",
        "    (x16,y16) = half_adder(x15,y13)\n",
        "    (m11,c11) = full_adder(x16,y14,c10)\n",
        "\n",
        "    ##column 13\n",
        "    (x17,y17) = COMPRESSORt(L[5][0],L[6][1],L[7][2],y15)\n",
        "    (m12,c12) = full_adder(x17,y16,c11)\n",
        "\n",
        "    ##column 14\n",
        "    (x18,y18) = half_adder(L[6][0],L[7][1])\n",
        "    (m13,c13) = full_adder(x18,y17,c12)\n",
        "\n",
        "    ##column 15\n",
        "    (m14,c14) = full_adder(L[7][0],c13,y18)\n",
        "\n",
        "    m15 = c14\n",
        "    m0 = m1 = m2 = m3 = 0\n",
        "    m4 = x21\n",
        "    out=[m15,m14,m13,m12,m11,m10,m9,m8,m7,m6,m5,m4,m3,m2,m1,m0]\n",
        "    result = ''.join(map(str, out))\n",
        "    return(int(result,2))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMIWSMSkXgWB"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kblZ7BP1D5iK"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate():\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import torchvision.transforms as transforms\n",
        "    from torchvision.datasets import MNIST\n",
        "    from torch.utils.data import DataLoader\n",
        "    import torch.ao.quantization as quant\n",
        "    import torchapprox.layers as tal\n",
        "    from torchapprox.utils import wrap_quantizable, get_approx_modules\n",
        "    import numpy as np\n",
        "    import time\n",
        "    import torch.nn.functional as F\n",
        "    from torch.utils.data import DataLoader, Subset\n",
        "    print(\"Checking for GPU availability...\")\n",
        "    # Check for GPU availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Define the custom CNN model\n",
        "    print(\"Defining the custom CNN model...\")\n",
        "    class LeNet5(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(LeNet5, self).__init__()\n",
        "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2)  # Output: 28x28\n",
        "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)  # Output: 14x14\n",
        "            self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)  # Output: 10x10\n",
        "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)  # Output: 5x5\n",
        "            self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Fully connected layer\n",
        "            self.fc2 = nn.Linear(120, 84)\n",
        "            self.fc3 = nn.Linear(84, 10)  # 10 output classes (digits 0-9)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = self.pool1(x)\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x = self.pool2(x)\n",
        "            x = x.view(-1, 16 * 5 * 5)  # Flatten for FC layers\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)  # No activation (CrossEntropyLoss includes softmax)\n",
        "            return x\n",
        "    # print(\"Initializing custom CNN model...\")\n",
        "    # Initialize the model\n",
        "    model = LeNet5()\n",
        "    model.to(device)\n",
        "    # print(\"Model initialized successfully!\")\n",
        "\n",
        "    # print(\"Wrapping layers for quantization...\")\n",
        "    # Wrap layers for quantization\n",
        "    wrap_quantizable(model)\n",
        "    # print(\"Wrapping completed.\")\n",
        "\n",
        "    # print(\"Preparing the model for QAT...\")\n",
        "    # Prepare model for Quantization-Aware Training (QAT)\n",
        "    quant.prepare_qat(model, tal.layer_mapping_dict(), inplace=True)\n",
        "    # print(\"Model prepared for QAT.\")\n",
        "\n",
        "    # print(\"Preparing MNIST dataset...\")\n",
        "    # Set up the dataset and data loaders\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
        "    ])\n",
        "    # train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    # num_samples = 20000\n",
        "    # indices = torch.randperm(len(train_dataset))[:num_samples]\n",
        "\n",
        "    # Use Subset to create a smaller dataset\n",
        "    # subset_train_dataset = Subset(train_dataset, indices)\n",
        "    # train_loader = DataLoader(subset_train_dataset, batch_size=256 * 4, shuffle=True, pin_memory=True, num_workers=8)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=256 * 4 * 4, shuffle=False, pin_memory=True, num_workers=8)\n",
        "    # print(\"Dataset and data loaders ready.\")\n",
        "\n",
        "    # print(\"Setting up the loss function and optimizer...\")\n",
        "    # Set up the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    # print(\"Loss function and optimizer set up.\")\n",
        "\n",
        "    # Generate the Lookup Table (LUT) for approximate multiplication\n",
        "    from temp import multiply2  # Import the multiply2 function from temp.py\n",
        "\n",
        "    def apply_bias(value):\n",
        "        return value if value < 128 else value - 256\n",
        "\n",
        "    lut_size = 256\n",
        "    lut = np.zeros((lut_size, lut_size), dtype=np.int16)\n",
        "\n",
        "    for i in range(lut_size):\n",
        "        for j in range(lut_size):\n",
        "            x_biased = apply_bias(i)\n",
        "            y_biased = apply_bias(j)\n",
        "            product = multiply2(abs(x_biased * 2), abs(y_biased * 2))\n",
        "            if (i >= 128 and j < 128) or (i < 128 and j >= 128):\n",
        "                product = -product\n",
        "            lut[i][j] = product / 4\n",
        "\n",
        "    # Function to set approximate forward pass\n",
        "    def set_approx_forward(model, lut):\n",
        "        for name, module in get_approx_modules(model):\n",
        "            if hasattr(module, 'approx_fwd'):\n",
        "                module.lut = lut\n",
        "                module.inference_mode = tal.InferenceMode.APPROXIMATE\n",
        "                # print(f\"Set approx_fwd with LUT for module: {name}\")\n",
        "            else:\n",
        "                continue\n",
        "                # print(f\"Module {name} does not support approx_fwd.\")\n",
        "\n",
        "    # Switch to approximate multiplication mode\n",
        "    set_approx_forward(model, lut)\n",
        "\n",
        "\n",
        "    # Fine-tune the model with approximate multiplication\n",
        "\n",
        "    #load the model -\n",
        "    model.load_state_dict(torch.load('lenet_mnist_final_CNN.pth'))\n",
        "\n",
        "    # print(\"Validating the model...\")\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    # print(f\"Validation complete. Accuracy on test set: {accuracy:.2f}%\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# accuracy = train_and_evaluate()\n",
        "# print(f\"Final accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4o6GawzCWQg"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# Define the compressor functions\n",
        "compressor_variants = [\n",
        "    \"ahmad\", \"akbari\", \"meo\", \"venka\", \"yang\", \"momeni\",\n",
        "    \"AC6G1\", \"AC6G2\", \"AC6G3\", \"AC6G4\", \"AC6G5\", \"AC6G6\",\n",
        "    \"AC6G7\", \"AC6G8\", \"AC6G9\", \"AC6G10\", \"AC6G11\", \"AC6G12\",\n",
        "    \"ACFGI1\", \"ACFGII1\", \"ACFGII10\", \"app_compressor\"\n",
        "]\n",
        "\n",
        "index_to_letter = string.ascii_lowercase\n",
        "\n",
        "def generate_code(vector):\n",
        "    \"\"\"\n",
        "    Replaces the compressor calls in column 8 based on the input vector.\n",
        "\n",
        "    Args:\n",
        "        vector (list): A list of size 3 indicating which compressor to use for each position.\n",
        "    Returns:\n",
        "        str: Modified Python code.\n",
        "    \"\"\"\n",
        "    if len(vector) != 20:\n",
        "        raise ValueError(\"Input vector must be of size 3.\")\n",
        "\n",
        "    modified_code = base_code\n",
        "\n",
        "    # Replace each COMPRESSOR placeholder with a variant from the combination\n",
        "    for idx, compressor_idx in enumerate(vector):\n",
        "        compressor_name = compressor_variants[compressor_idx]\n",
        "        placeholder = f\"COMPRESSOR{index_to_letter[idx]}\"\n",
        "        # print(f\"Replacing {placeholder} with {compressor_name}\")\n",
        "        modified_code = modified_code.replace(placeholder, compressor_name)\n",
        "\n",
        "\n",
        "    return modified_code\n",
        "\n",
        "\n",
        "# vector = [0, 12, 2, 3, 4, 5, 6, 7, 8, 9, 20, 11, 12, 13, 14, 15, 16, 17, 18, 21]\n",
        "# code = generate_code(vector)\n",
        "# print(code)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LCSXbp6E5Yu"
      },
      "outputs": [],
      "source": [
        "def MRED(x):\n",
        "    print(\"MRED: \", x)\n",
        "    gencode = generate_code(x)\n",
        "\n",
        "    # Write the code to temp.py\n",
        "    with open(\"temp.py\", \"w\") as file:\n",
        "        file.write(gencode)\n",
        "    print(\"Code written to temp.py\")\n",
        "\n",
        "    # Dynamically load temp.py\n",
        "    import importlib.util\n",
        "    import sys, os\n",
        "\n",
        "    if \"temp\" in sys.modules:\n",
        "        del sys.modules[\"temp\"]\n",
        "\n",
        "    spec = importlib.util.spec_from_file_location(\"temp\", os.path.abspath(\"temp.py\"))\n",
        "    temp = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(temp)\n",
        "\n",
        "    multiply2 = temp.multiply2\n",
        "\n",
        "    # Run MRED\n",
        "    bit_width = 8\n",
        "    samples = [(a, b) for a in range(2**bit_width) for b in range(2**bit_width)]\n",
        "\n",
        "    total_error = 0\n",
        "    count = 0\n",
        "\n",
        "    for a, b in samples:\n",
        "        approx_result = multiply2(a, b)\n",
        "        exact_result = a * b\n",
        "        if exact_result != 0:\n",
        "            total_error += abs(approx_result - exact_result) / exact_result\n",
        "        count += 1\n",
        "\n",
        "    MRED_sampled = total_error / count if count > 0 else 0\n",
        "    print(\"MRED_sampled: \", MRED_sampled)\n",
        "\n",
        "    return MRED_sampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "OonWtbOkHY1e",
        "outputId": "06614a82-91a6-4934-f49e-35f426555c21"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5f275372-a4f8-4c22-8e81-c89a35ac1db5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5f275372-a4f8-4c22-8e81-c89a35ac1db5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving parameters.yaml to parameters.yaml\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ_TP1O0IW1k"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "def compute_latencies(params, x):\n",
        "    comp_counts = [2, 2, 3, 3, 3, 3, 2, 1, 1]\n",
        "    latencies = {}\n",
        "    x_index = 0\n",
        "\n",
        "    for col, count in zip(range(5, 14), comp_counts):\n",
        "        if count == 3:\n",
        "            lat1 = params['compressors'][x[x_index]]['latency']\n",
        "            x_index += 1\n",
        "            lat2 = params['compressors'][x[x_index]]['latency']\n",
        "            x_index += 1\n",
        "            lat3 = params['compressors'][x[x_index]]['latency']\n",
        "            x_index += 1\n",
        "            latency = max(lat1, lat2) + lat3\n",
        "        else:\n",
        "            latency = 0\n",
        "            for _ in range(count):\n",
        "                latency += params['compressors'][x[x_index]]['latency']\n",
        "                x_index += 1\n",
        "\n",
        "        latencies[col] = latency\n",
        "    return latencies\n",
        "\n",
        "def optimizationParam(x):\n",
        "    with open(\"parameters.yaml\", \"r\") as file:\n",
        "        params = yaml.safe_load(file)\n",
        "\n",
        "    total_latency = compute_latencies(params, x)\n",
        "    total_power = 0\n",
        "\n",
        "    for index in x:\n",
        "        if 0 <= index < len(params['compressors']):\n",
        "            compressor = params['compressors'][index]\n",
        "            total_power += compressor['power']\n",
        "        else:\n",
        "            raise ValueError(f\"Index {index} is out of range.\")\n",
        "\n",
        "    return max(total_latency.values()), total_power\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMJCmhk3IdmF"
      },
      "outputs": [],
      "source": [
        "def multi_objective_function(x):\n",
        "    \"\"\"\n",
        "    This function calculates four objectives: MRED, Latency, Area, and Power.\n",
        "    The calculation is arbitrary for demonstration purposes. Replace these with your actual equations.\n",
        "    \"\"\"\n",
        "    print(x, type(x))\n",
        "    # Replace these with actual formulas for MRED, Latency, Area, and Power\n",
        "    # from mred import MRED\n",
        "    # from accuracy import train_and_evaluate\n",
        "    # from optimizationParams import optimizationParam\n",
        "    error = MRED(x)  # Example: Sum of squares\n",
        "\n",
        "    final_acc =  train_and_evaluate()/100\n",
        "    # Calculate Latency, Area, and Power using the optimizationPram function\n",
        "    Latency, Power = optimizationParam(x)\n",
        "    print(error, Latency, Power, final_acc)\n",
        "    return error, Latency, Power, final_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKKMLR2lJoLE",
        "outputId": "60da870f-2ed8-44c5-f86f-5b498d92fc47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pymoo --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NoGpQl4wI5Vj",
        "outputId": "cf2526e6-7d9a-4fd5-f0ea-c24a4f1b40f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input: [19 15  4 10  8  1  3  4  7  3  6  8 17 20 20 18  1  9 15  4]\n",
            "[19 15  4 10  8  1  3  4  7  3  6  8 17 20 20 18  1  9 15  4] <class 'numpy.ndarray'>\n",
            "MRED:  [19 15  4 10  8  1  3  4  7  3  6  8 17 20 20 18  1  9 15  4]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.32685900388643346\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.32685900388643346 0.22612 4.371740000000002 0.7565999999999999\n",
            "Processing input: [ 7  5 10 21 11 19  1  6 21 13  2 15 21  1  4 20 20 14 17 12]\n",
            "[ 7  5 10 21 11 19  1  6 21 13  2 15 21  1  4 20 20 14 17 12] <class 'numpy.ndarray'>\n",
            "MRED:  [ 7  5 10 21 11 19  1  6 21 13  2 15 21  1  4 20 20 14 17 12]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.13270275685987146\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.13270275685987146 0.19636 3.2491800000000004 0.868\n",
            "Processing input: [ 3 21 13 15  6  2  2  9  3 16 14 19  4  4 11 15 12  5 19 17]\n",
            "[ 3 21 13 15  6  2  2  9  3 16 14 19  4  4 11 15 12  5 19 17] <class 'numpy.ndarray'>\n",
            "MRED:  [ 3 21 13 15  6  2  2  9  3 16 14 19  4  4 11 15 12  5 19 17]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.6107916527489229\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.6107916527489229 0.30874 4.34198 0.4844\n",
            "Processing input: [ 1  6 11  7  2 16 15 11  8 19  4 16 14 13  8 13 10  9  1  3]\n",
            "[ 1  6 11  7  2 16 15 11  8 19  4 16 14 13  8 13 10  9  1  3] <class 'numpy.ndarray'>\n",
            "MRED:  [ 1  6 11  7  2 16 15 11  8 19  4 16 14 13  8 13 10  9  1  3]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.08580138071505768\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.08580138071505768 0.19431 3.58774 0.9031\n",
            "Processing input: [12  5 10  4 11 13 10  6 11  4 20  4 15 16  7 21  9  7  1 10]\n",
            "[12  5 10  4 11 13 10  6 11  4 20  4 15 16  7 21  9  7  1 10] <class 'numpy.ndarray'>\n",
            "MRED:  [12  5 10  4 11 13 10  6 11  4 20  4 15 16  7 21  9  7  1 10]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.09155643136322861\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.09155643136322861 0.1665 3.86566 0.9053\n",
            "Processing input: [12 18 12  3 16 11  5  7 20 14 20  9 18 10 13  8  3  2  3 13]\n",
            "[12 18 12  3 16 11  5  7 20 14 20  9 18 10 13  8  3  2  3 13] <class 'numpy.ndarray'>\n",
            "MRED:  [12 18 12  3 16 11  5  7 20 14 20  9 18 10 13  8  3  2  3 13]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.20673933418041976\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.20673933418041976 0.25393 3.2642599999999993 0.8758\n",
            "Processing input: [10 14 18  5  4  3 11 11 18 10  9  3 11 21  2  5 16 16  6 20]\n",
            "[10 14 18  5  4  3 11 11 18 10  9  3 11 21  2  5 16 16  6 20] <class 'numpy.ndarray'>\n",
            "MRED:  [10 14 18  5  4  3 11 11 18 10  9  3 11 21  2  5 16 16  6 20]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.45677083170586785\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.45677083170586785 0.22423 3.7820799999999997 0.3611\n",
            "Processing input: [20  1 21 11 15  3 11  1  3 16  5 17  8  2 11  6 13  3 14 15]\n",
            "[20  1 21 11 15  3 11  1  3 16  5 17  8  2 11  6 13  3 14 15] <class 'numpy.ndarray'>\n",
            "MRED:  [20  1 21 11 15  3 11  1  3 16  5 17  8  2 11  6 13  3 14 15]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.18075439325623482\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.18075439325623482 0.19431 4.1531400000000005 0.8835\n",
            "Processing input: [15 13 17  1  2 15 13  8  4 12  5  9  1  5 12 15  5  3 21  8]\n",
            "[15 13 17  1  2 15 13  8  4 12  5  9  1  5 12 15  5  3 21  8] <class 'numpy.ndarray'>\n",
            "MRED:  [15 13 17  1  2 15 13  8  4 12  5  9  1  5 12 15  5  3 21  8]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.9251987539776393\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.9251987539776393 0.19431 4.68304 0.1193\n",
            "Processing input: [ 2 16 18  8 11 12  1 19  5  7  1  2 13 18 16  3 13 19 20 10]\n",
            "[ 2 16 18  8 11 12  1 19  5  7  1  2 13 18 16  3 13 19 20 10] <class 'numpy.ndarray'>\n",
            "MRED:  [ 2 16 18  8 11 12  1 19  5  7  1  2 13 18 16  3 13 19 20 10]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.4518850584481689\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.4518850584481689 0.19431 3.391840000000001 0.5690999999999999\n",
            "Processing input: [ 2  6  4  7  5 16  7  6 16  5  8 14 16 11  5 13 10 20  5  3]\n",
            "[ 2  6  4  7  5 16  7  6 16  5  8 14 16 11  5 13 10 20  5  3] <class 'numpy.ndarray'>\n",
            "MRED:  [ 2  6  4  7  5 16  7  6 16  5  8 14 16 11  5 13 10 20  5  3]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  1.4092778387518132\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "1.4092778387518132 0.19431 4.32522 0.11349999999999999\n",
            "Processing input: [15 12  2  4  4  8  4 20  1 14  2 14  3  7  4 10 11  2  6 20]\n",
            "[15 12  2  4  4  8  4 20  1 14  2 14  3  7  4 10 11  2  6 20] <class 'numpy.ndarray'>\n",
            "MRED:  [15 12  2  4  4  8  4 20  1 14  2 14  3  7  4 10 11  2  6 20]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.18993677272930848\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.18993677272930848 0.28093 5.30154 0.5824\n",
            "Processing input: [18  0 15  3 16 10 16  1 14  0  6 15 13  6 13  3 16  5  8  5]\n",
            "[18  0 15  3 16 10 16  1 14  0  6 15 13  6 13  3 16  5  8  5] <class 'numpy.ndarray'>\n",
            "MRED:  [18  0 15  3 16 10 16  1 14  0  6 15 13  6 13  3 16  5  8  5]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  2.704517821168879\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "2.704517821168879 0.1395 3.69522 0.11349999999999999\n",
            "Processing input: [18 12  9 10  0 17  1 16  2 19  8  6 18  9 16  7  7 13 10 14]\n",
            "[18 12  9 10  0 17  1 16  2 19  8  6 18  9 16  7  7 13 10 14] <class 'numpy.ndarray'>\n",
            "MRED:  [18 12  9 10  0 17  1 16  2 19  8  6 18  9 16  7  7 13 10 14]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.2120444593069493\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.2120444593069493 0.15447 2.2879399999999994 0.8861\n",
            "Processing input: [ 2 20  6  9 21 14  9 20  9 17  3  9 19  7  9 17 21  6  7 10]\n",
            "[ 2 20  6  9 21 14  9 20  9 17  3  9 19  7  9 17 21  6  7 10] <class 'numpy.ndarray'>\n",
            "MRED:  [ 2 20  6  9 21 14  9 20  9 17  3  9 19  7  9 17 21  6  7 10]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.09928833752000217\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.09928833752000217 0.15447 2.1650399999999994 0.9045000000000001\n",
            "Processing input: [ 6  8  9  5 17  9 18 15 18 13 14  8 13 15 18  6  6 20  4  0]\n",
            "[ 6  8  9  5 17  9 18 15 18 13 14  8 13 15 18  6  6 20  4  0] <class 'numpy.ndarray'>\n",
            "MRED:  [ 6  8  9  5 17  9 18 15 18 13 14  8 13 15 18  6  6 20  4  0]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.43193677350081544\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.43193677350081544 0.12656 2.3967799999999992 0.659\n",
            "Processing input: [16 17  6 19 17  6  0 18 20  8  4  6  9  8 15  5  2 18 15  6]\n",
            "[16 17  6 19 17  6  0 18 20  8  4  6  9  8 15  5  2 18 15  6] <class 'numpy.ndarray'>\n",
            "MRED:  [16 17  6 19 17  6  0 18 20  8  4  6  9  8 15  5  2 18 15  6]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.838299227971304\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.838299227971304 0.1665 2.71552 0.1801\n",
            "Processing input: [12  9  1 12  9 10 14 16  0 21  3 10  5 14  8 11 19  9 19 18]\n",
            "[12  9  1 12  9 10 14 16  0 21  3 10  5 14  8 11 19  9 19 18] <class 'numpy.ndarray'>\n",
            "MRED:  [12  9  1 12  9 10 14 16  0 21  3 10  5 14  8 11 19  9 19 18]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  2.3493271285915647\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "2.3493271285915647 0.16942000000000002 2.6546000000000007 0.11349999999999999\n",
            "Processing input: [18 14  5 18 15 17  7  2  8  5  3 16  5  5 19  0 12 16  3  5]\n",
            "[18 14  5 18 15 17  7  2  8  5  3 16  5  5 19  0 12 16  3  5] <class 'numpy.ndarray'>\n",
            "MRED:  [18 14  5 18 15 17  7  2  8  5  3 16  5  5 19  0 12 16  3  5]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  2.6068440928405727\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "2.6068440928405727 0.22423 4.460380000000001 0.11349999999999999\n",
            "Processing input: [ 4 10  7  5 20  7 10 21  7 12 17 11 20  5 10 15  2 12  7  2]\n",
            "[ 4 10  7  5 20  7 10 21  7 12 17 11 20  5 10 15  2 12  7  2] <class 'numpy.ndarray'>\n",
            "MRED:  [ 4 10  7  5 20  7 10 21  7 12 17 11 20  5 10 15  2 12  7  2]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.30561381680787086\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.30561381680787086 0.19431 3.4345000000000003 0.8115000000000001\n",
            "Processing input: [19 15  2 13 14  4  2 12  1  6  0  2  3  8  3  4 19 11  9 12]\n",
            "[19 15  2 13 14  4  2 12  1  6  0  2  3  8  3  4 19 11  9 12] <class 'numpy.ndarray'>\n",
            "MRED:  [19 15  2 13 14  4  2 12  1  6  0  2  3  8  3  4 19 11  9 12]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.10947968428602761\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.10947968428602761 0.28093 4.77154 0.8417\n",
            "Processing input: [ 4  3  5 14 19 10  6 12 15 11 10 18  1 15 10  8 17  6 11  7]\n",
            "[ 4  3  5 14 19 10  6 12 15 11 10 18  1 15 10  8 17  6 11  7] <class 'numpy.ndarray'>\n",
            "MRED:  [ 4  3  5 14 19 10  6 12 15 11 10 18  1 15 10  8 17  6 11  7]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.20685079687258\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.20685079687258 0.22612 3.1978400000000007 0.8606\n",
            "Processing input: [10 13 19 13 12 14 19 10  9  8 12  3  7 11  9  1 18  6  7  4]\n",
            "[10 13 19 13 12 14 19 10  9  8 12  3  7 11  9  1 18  6  7  4] <class 'numpy.ndarray'>\n",
            "MRED:  [10 13 19 13 12 14 19 10  9  8 12  3  7 11  9  1 18  6  7  4]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.5681307594550272\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.5681307594550272 0.1395 2.7579399999999996 0.4921\n",
            "Processing input: [17 16  1  9 18  3 15  8 14 10 19 17  8 15  3 11  8  4 17 17]\n",
            "[17 16  1  9 18  3 15  8 14 10 19 17  8 15  3 11  8  4 17 17] <class 'numpy.ndarray'>\n",
            "MRED:  [17 16  1  9 18  3 15  8 14 10 19 17  8 15  3 11  8  4 17 17]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.11750455331822443\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.11750455331822443 0.1665 3.2278399999999996 0.9028\n",
            "Processing input: [ 3  5 14  1 13 13  8  1  9  8 19 11  9  7 13  1  0  4 18 16]\n",
            "[ 3  5 14  1 13 13  8  1  9  8 19 11  9  7 13  1  0  4 18 16] <class 'numpy.ndarray'>\n",
            "MRED:  [ 3  5 14  1 13 13  8  1  9  8 19 11  9  7 13  1  0  4 18 16]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  1.1072110441407237\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "1.1072110441407237 0.16942000000000002 3.944060000000001 0.1686\n",
            "Processing input: [ 8  2  7  6  3 18 17 14  4 19 11 20  6 18  4  2 10 10 13 17]\n",
            "[ 8  2  7  6  3 18 17 14  4 19 11 20  6 18  4  2 10 10 13 17] <class 'numpy.ndarray'>\n",
            "MRED:  [ 8  2  7  6  3 18 17 14  4 19 11 20  6 18  4  2 10 10 13 17]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.3482851015802303\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.3482851015802303 0.28093 3.4742600000000006 0.8371999999999999\n",
            "Processing input: [ 1 19  3 16  8  0 20  9 11  3 10 13 18 16 15  6 19 20  9 11]\n",
            "[ 1 19  3 16  8  0 20  9 11  3 10 13 18 16 15  6 19 20  9 11] <class 'numpy.ndarray'>\n",
            "MRED:  [ 1 19  3 16  8  0 20  9 11  3 10 13 18 16 15  6 19 20  9 11]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.255962745517206\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.255962745517206 0.1395 2.5443600000000006 0.8478\n",
            "Processing input: [ 6  9  8 17  7 15 17 12  5 14 13  3 14 18  6  8  7  1 18 11]\n",
            "[ 6  9  8 17  7 15 17 12  5 14 13  3 14 18  6  8  7  1 18 11] <class 'numpy.ndarray'>\n",
            "MRED:  [ 6  9  8 17  7 15 17 12  5 14 13  3 14 18  6  8  7  1 18 11]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  1.4269455995196156\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "1.4269455995196156 0.1395 2.6991 0.1159\n",
            "Processing input: [19 20  1 20  9 11 17  3 16  4 16  1 20 12  2 19 18 17  5 18]\n",
            "[19 20  1 20  9 11 17  3 16  4 16  1 20 12  2 19 18 17  5 18] <class 'numpy.ndarray'>\n",
            "MRED:  [19 20  1 20  9 11 17  3 16  4 16  1 20 12  2 19 18 17  5 18]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  3.8727556013741635\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "3.8727556013741635 0.22612 3.389520000000001 0.11349999999999999\n",
            "Processing input: [21  7 19 10 15 15  5  4 10 18 18  4 12  1  9 14  5 19  1 15]\n",
            "[21  7 19 10 15 15  5  4 10 18 18  4 12  1  9 14  5 19  1 15] <class 'numpy.ndarray'>\n",
            "MRED:  [21  7 19 10 15 15  5  4 10 18 18  4 12  1  9 14  5 19  1 15]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  1.3074575221567453\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "1.3074575221567453 0.1665 3.7709200000000003 0.11349999999999999\n",
            "Processing input: [ 4  2 17 15  1 20  4  5 13  2 21  5 20 17 12 11 14 10 16  9]\n",
            "[ 4  2 17 15  1 20  4  5 13  2 21  5 20 17 12 11 14 10 16  9] <class 'numpy.ndarray'>\n",
            "MRED:  [ 4  2 17 15  1 20  4  5 13  2 21  5 20 17 12 11 14 10 16  9]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.14237307897086315\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.14237307897086315 0.28093 4.261980000000001 0.8663\n",
            "Processing input: [11  8 16 12 18 20 20  5  2  9 13 14 19  1 20  4  4 18 11  9]\n",
            "[11  8 16 12 18 20 20  5  2  9 13 14 19  1 20  4  4 18 11  9] <class 'numpy.ndarray'>\n",
            "MRED:  [11  8 16 12 18 20 20  5  2  9 13 14 19  1 20  4  4 18 11  9]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.6654749341378667\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.6654749341378667 0.19431 3.27194 0.3267\n",
            "Processing input: [13 18  0  2 14  8 16 13 17  1  6 11 17 17  6  2  8  7 20  6]\n",
            "[13 18  0  2 14  8 16 13 17  1  6 11 17 17  6  2  8  7 20  6] <class 'numpy.ndarray'>\n",
            "MRED:  [13 18  0  2 14  8 16 13 17  1  6 11 17 17  6  2  8  7 20  6]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.1580009236868485\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.1580009236868485 0.19431 2.6978400000000002 0.7653\n",
            "Processing input: [15 11  9  0 20  5 13 13 12 10 20  6 10  8 17  2 12 11 17 19]\n",
            "[15 11  9  0 20  5 13 13 12 10 20  6 10  8 17  2 12 11 17 19] <class 'numpy.ndarray'>\n",
            "MRED:  [15 11  9  0 20  5 13 13 12 10 20  6 10  8 17  2 12 11 17 19]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.17675052644938732\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.17675052644938732 0.19431 2.30794 0.5136\n",
            "Processing input: [ 5 20 20  1  6  2 12 14 12  1 13 18  2 14 18 19 11 12  8 14]\n",
            "[ 5 20 20  1  6  2 12 14 12  1 13 18  2 14 18 19 11 12  8 14] <class 'numpy.ndarray'>\n",
            "MRED:  [ 5 20 20  1  6  2 12 14 12  1 13 18  2 14 18 19 11 12  8 14]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.7022117616463983\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.7022117616463983 0.19431 3.013100000000001 0.3765\n",
            "Processing input: [ 8  1 20  2 10  7  6  3  6 15 17  8  6  6 15 19  1  3  2  6]\n",
            "[ 8  1 20  2 10  7  6  3  6 15 17  8  6  6 15 19  1  3  2  6] <class 'numpy.ndarray'>\n",
            "MRED:  [ 8  1 20  2 10  7  6  3  6 15 17  8  6  6 15 19  1  3  2  6]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.08909768816079697\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.08909768816079697 0.16605999999999999 3.6953199999999997 0.909\n",
            "Processing input: [14 19 11 17 21 19  8 19  6 15 11  0  0  3 19  7 15  0  4  2]\n",
            "[14 19 11 17 21 19  8 19  6 15 11  0  0  3 19  7 15  0  4  2] <class 'numpy.ndarray'>\n",
            "MRED:  [14 19 11 17 21 19  8 19  6 15 11  0  0  3 19  7 15  0  4  2]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.0866421378172788\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.0866421378172788 0.15437 2.8809199999999997 0.9116\n",
            "Processing input: [13 12  3  4  3 17 15 15 19  7 17 17 10 20  1 18  3 18 12 19]\n",
            "[13 12  3  4  3 17 15 15 19  7 17 17 10 20  1 18  3 18 12 19] <class 'numpy.ndarray'>\n",
            "MRED:  [13 12  3  4  3 17 15 15 19  7 17 17 10 20  1 18  3 18 12 19]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.8598294553163818\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.8598294553163818 0.22612 3.333100000000001 0.1137\n",
            "Processing input: [11 11 17 16 19  9  4 17 14 11 15 20 15 11 21 16 16 21 20  1]\n",
            "[11 11 17 16 19  9  4 17 14 11 15 20 15 11 21 16 16 21 20  1] <class 'numpy.ndarray'>\n",
            "MRED:  [11 11 17 16 19  9  4 17 14 11 15 20 15 11 21 16 16 21 20  1]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.15908839367149033\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.15908839367149033 0.1665 2.2950399999999993 0.8271\n",
            "Processing input: [ 6  3 13 14 13 20  9 17 15  9 15 12  6 10 19 21  6 12  9 13]\n",
            "[ 6  3 13 14 13 20  9 17 15  9 15 12  6 10 19 21  6 12  9 13] <class 'numpy.ndarray'>\n",
            "MRED:  [ 6  3 13 14 13 20  9 17 15  9 15 12  6 10 19 21  6 12  9 13]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.09693695556453265\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.09693695556453265 0.1395 1.979539999999999 0.9087000000000001\n",
            "Processing input: [20  7 15 20  2 18 12  2 11 12 12 10 14 20  1  3  1  1 10 16]\n",
            "[20  7 15 20  2 18 12  2 11 12 12 10 14 20  1  3  1  1 10 16] <class 'numpy.ndarray'>\n",
            "MRED:  [20  7 15 20  2 18 12  2 11 12 12 10 14 20  1  3  1  1 10 16]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.11533206856444234\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.11533206856444234 0.19431 3.462999999999999 0.8856999999999999\n",
            "Processing input: [ 7  9 14 15  7  2 14  7  6  6  9 12  2 17 18 18 18 14 12 12]\n",
            "[ 7  9 14 15  7  2 14  7  6  6  9 12  2 17 18 18 18 14 12 12] <class 'numpy.ndarray'>\n",
            "MRED:  [ 7  9 14 15  7  2 14  7  6  6  9 12  2 17 18 18 18 14 12 12]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.8751496001260419\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.8751496001260419 0.19431 2.1891999999999996 0.37\n",
            "Processing input: [ 0  3 14 12  4  1 12 10 10 20  7 19  8 13  7 10 15 16  2 15]\n",
            "[ 0  3 14 12  4  1 12 10 10 20  7 19  8 13  7 10 15 16  2 15] <class 'numpy.ndarray'>\n",
            "MRED:  [ 0  3 14 12  4  1 12 10 10 20  7 19  8 13  7 10 15 16  2 15]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.1089027232059084\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.1089027232059084 0.1665 3.2565800000000014 0.9079999999999999\n",
            "Processing input: [17 17 19 18 13 21 19 17 18  1 12 21  4 19  1 17  6 13  5  4]\n",
            "[17 17 19 18 13 21 19 17 18  1 12 21  4 19  1 17  6 13  5  4] <class 'numpy.ndarray'>\n",
            "MRED:  [17 17 19 18 13 21 19 17 18  1 12 21  4 19  1 17  6 13  5  4]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  1.1406614710452017\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "1.1406614710452017 0.16855 3.2889399999999998 0.1246\n",
            "Processing input: [16 13 12 18 10  4  3 18  2 17  2  1 11  9  5 14 15  8 13 20]\n",
            "[16 13 12 18 10  4  3 18  2 17  2  1 11  9  5 14 15  8 13 20] <class 'numpy.ndarray'>\n",
            "MRED:  [16 13 12 18 10  4  3 18  2 17  2  1 11  9  5 14 15  8 13 20]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.5057585416420679\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.5057585416420679 0.22612 3.7441600000000004 0.3094\n",
            "Processing input: [ 9  4  6 16  0  7  9  0  4 20 18 13  1 13  7 12 13  2 12 21]\n",
            "[ 9  4  6 16  0  7  9  0  4 20 18 13  1 13  7 12 13  2 12 21] <class 'numpy.ndarray'>\n",
            "MRED:  [ 9  4  6 16  0  7  9  0  4 20 18 13  1 13  7 12 13  2 12 21]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.1728867712192475\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.1728867712192475 0.19431 3.4232400000000007 0.889\n",
            "Processing input: [ 9  1  4 20 17  6 18  3 19  3 16  1  8  0 17 13  3 15 16  1]\n",
            "[ 9  1  4 20 17  6 18  3 19  3 16  1  8  0 17 13  3 15 16  1] <class 'numpy.ndarray'>\n",
            "MRED:  [ 9  1  4 20 17  6 18  3 19  3 16  1  8  0 17 13  3 15 16  1]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.09604716535554025\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.09604716535554025 0.19912 4.261640000000001 0.9155\n",
            "Processing input: [ 5 17 16 11  9 13  7 14 13 17 10  7 11  2  0 16 20 16  0  7]\n",
            "[ 5 17 16 11  9 13  7 14 13 17 10  7 11  2  0 16 20 16  0  7] <class 'numpy.ndarray'>\n",
            "MRED:  [ 5 17 16 11  9 13  7 14 13 17 10  7 11  2  0 16 20 16  0  7]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.12201580663310026\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.12201580663310026 0.19431 2.579 0.8492000000000001\n",
            "Processing input: [14 10 12  7  7 12 19 10 20 18  7 21  4  3 14  9  5  8 14  8]\n",
            "[14 10 12  7  7 12 19 10 20 18  7 21  4  3 14  9  5  8 14  8] <class 'numpy.ndarray'>\n",
            "MRED:  [14 10 12  7  7 12 19 10 20 18  7 21  4  3 14  9  5  8 14  8]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.6242781810057325\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.6242781810057325 0.16855 2.73586 0.4081\n",
            "Processing input: [ 8  7  8  8  5  5 21 21 17  5  1  5 16  3 14  9  8 15 15  2]\n",
            "[ 8  7  8  8  5  5 21 21 17  5  1  5 16  3 14  9  8 15 15  2] <class 'numpy.ndarray'>\n",
            "MRED:  [ 8  7  8  8  5  5 21 21 17  5  1  5 16  3 14  9  8 15 15  2]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.27381203826100126\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.27381203826100126 0.15437 4.1622200000000005 0.8503000000000001\n",
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      4 |             - |             -\n",
            "Processing input: [12  8  1 12  9 10 14 16  0 21 20  9  5 14  8 11 20  9 19 18]\n",
            "[12  8  1 12  9 10 14 16  0 21 20  9  5 14  8 11 20  9 19 18] <class 'numpy.ndarray'>\n",
            "MRED:  [12  8  1 12  9 10 14 16  0 21 20  9  5 14  8 11 20  9 19 18]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  2.3576425072790945\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "2.3576425072790945 0.10980000000000001 2.1847000000000003 0.11349999999999999\n",
            "Processing input: [11  3 17 16 19  9  4 17 14 11  7 19 15 11 21 16 16 21 19  1]\n",
            "[11  3 17 16 19  9  4 17 14 11  7 19 15 11 21 16 16 21 19  1] <class 'numpy.ndarray'>\n",
            "MRED:  [11  3 17 16 19  9  4 17 14 11  7 19 15 11 21 16 16 21 19  1]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.15729755864259432\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.15729755864259432 0.1665 2.6737799999999994 0.8345999999999999\n",
            "Processing input: [ 9  4  6 16  0  7  9  0  4 20 18 13  1 13  7 12 13  2 12 21]\n",
            "[ 9  4  6 16  0  7  9  0  4 20 18 13  1 13  7 12 13  2 12 21] <class 'numpy.ndarray'>\n",
            "MRED:  [ 9  4  6 16  0  7  9  0  4 20 18 13  1 13  7 12 13  2 12 21]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.1728867712192475\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.1728867712192475 0.19431 3.4232400000000007 0.889\n",
            "Processing input: [16  9  6 19 17  6  0 18 20  8  4 12  9  8 15  5  2 18 15 12]\n",
            "[16  9  6 19 17  6  0 18 20  8  4 12  9  8 15  5  2 18 15 12] <class 'numpy.ndarray'>\n",
            "MRED:  [16  9  6 19 17  6  0 18 20  8  4 12  9  8 15  5  2 18 15 12]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.8299391132197103\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.8299391132197103 0.1665 2.71552 0.1157\n",
            "Processing input: [ 6 17 16 11 21 13  7 14 13 17 10  7 11  2  0 16 20 16  0  7]\n",
            "[ 6 17 16 11 21 13  7 14 13 17 10  7 11  2  0 16 20 16  0  7] <class 'numpy.ndarray'>\n",
            "MRED:  [ 6 17 16 11 21 13  7 14 13 17 10  7 11  2  0 16 20 16  0  7]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.12218321643556979\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.12218321643556979 0.19431 2.1881799999999996 0.8528\n",
            "Processing input: [15 11  9  0 20  1 13  6 12 10 20  6 16 16  7  2 12 11 17 20]\n",
            "[15 11  9  0 20  1 13  6 12 10 20  6 16 16  7  2 12 11 17 20] <class 'numpy.ndarray'>\n",
            "MRED:  [15 11  9  0 20  1 13  6 12 10 20  6 16 16  7  2 12 11 17 20]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.20634476432179358\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.20634476432179358 0.19431 2.28794 0.5788\n",
            "Processing input: [18 12  9 10  0 17  1 16  2 19  8  6 18  9 16  7  7 13 10 14]\n",
            "[18 12  9 10  0 17  1 16  2 19  8  6 18  9 16  7  7 13 10 14] <class 'numpy.ndarray'>\n",
            "MRED:  [18 12  9 10  0 17  1 16  2 19  8  6 18  9 16  7  7 13 10 14]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.2120444593069493\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.2120444593069493 0.15447 2.2879399999999994 0.8861\n",
            "Processing input: [ 8  2 10  7  3 19 18 14 21 19 11 20  6 18  4 18 11 14 13 11]\n",
            "[ 8  2 10  7  3 19 18 14 21 19 11 20  6 18  4 18 11 14 13 11] <class 'numpy.ndarray'>\n",
            "MRED:  [ 8  2 10  7  3 19 18 14 21 19 11 20  6 18  4 18 11 14 13 11]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.870079031083194\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.870079031083194 0.19431 2.43238 0.2778\n",
            "Processing input: [17 16  1  9 18  3 15  8  1 10 19 17  8 14  3 11  8  4 17 17]\n",
            "[17 16  1  9 18  3 15  8  1 10 19 17  8 14  3 11  8  4 17 17] <class 'numpy.ndarray'>\n",
            "MRED:  [17 16  1  9 18  3 15  8  1 10 19 17  8 14  3 11  8  4 17 17]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.11747067962511737\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.11747067962511737 0.1665 3.5565799999999994 0.8998999999999999\n",
            "Processing input: [ 8 13 19 13 12 14 19 10  9  5 11  3 16  3 14  1 18  6  7  4]\n",
            "[ 8 13 19 13 12 14 19 10  9  5 11  3 16  3 14  1 18  6  7  4] <class 'numpy.ndarray'>\n",
            "MRED:  [ 8 13 19 13 12 14 19 10  9  5 11  3 16  3 14  1 18  6  7  4]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.6046617326947352\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.6046617326947352 0.16605999999999999 3.48542 0.424\n",
            "Processing input: [14  1 15  2 16 10 16  1 14  1  6 15 13 17 13  3  8  5  8  5]\n",
            "[14  1 15  2 16 10 16  1 14  1  6 15 13 17 13  3  8  5  8  5] <class 'numpy.ndarray'>\n",
            "MRED:  [14  1 15  2 16 10 16  1 14  1  6 15 13 17 13  3  8  5  8  5]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  2.6973964038519758\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "2.6973964038519758 0.19431 4.20638 0.11349999999999999\n",
            "Processing input: [ 1 19  3 16  8  0 20 17 11  9 10 13  7 16 15 21 19 20  9 11]\n",
            "[ 1 19  3 16  8  0 20 17 11  9 10 13  7 16 15 21 19 20  9 11] <class 'numpy.ndarray'>\n",
            "MRED:  [ 1 19  3 16  8  0 20 17 11  9 10 13  7 16 15 21 19 20  9 11]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.15532222285041444\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.15532222285041444 0.1395 2.2146999999999997 0.8361\n",
            "Processing input: [ 4 10 15 20  2 18 10  2 11 12 12 11 14 20  1  3  1  1 10 16]\n",
            "[ 4 10 15 20  2 18 10  2 11 12 12 11 14 20  1  3  1  1 10 16] <class 'numpy.ndarray'>\n",
            "MRED:  [ 4 10 15 20  2 18 10  2 11 12 12 11 14 20  1  3  1  1 10 16]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.11343732161376854\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.11343732161376854 0.19431 4.0529 0.8811\n",
            "Processing input: [ 8  1 10  4 10 13  6  6  6 15 20  8  6 16 15 19  1  3  2 10]\n",
            "[ 8  1 10  4 10 13  6  6  6 15 20  8  6 16 15 19  1  3  2 10] <class 'numpy.ndarray'>\n",
            "MRED:  [ 8  1 10  4 10 13  6  6  6 15 20  8  6 16 15 19  1  3  2 10]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.09577243861702106\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.09577243861702106 0.1665 3.49658 0.9059999999999999\n",
            "Processing input: [ 6  8  9  5 17  9 18 14 18 13 14  8 13 16 18  6  7 20  4  0]\n",
            "[ 6  8  9  5 17  9 18 14 18 13 14  8 13 16 18  6  7 20  4  0] <class 'numpy.ndarray'>\n",
            "MRED:  [ 6  8  9  5 17  9 18 14 18 13 14  8 13 16 18  6  7 20  4  0]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.4330822581663375\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.4330822581663375 0.12656 2.3967799999999992 0.6425\n",
            "Processing input: [ 9 20  6 10  2 14  8 19  4 17 11 19  6  7  9 17 21  6  7 10]\n",
            "[ 9 20  6 10  2 14  8 19  4 17 11 19  6  7  9 17 21  6  7 10] <class 'numpy.ndarray'>\n",
            "MRED:  [ 9 20  6 10  2 14  8 19  4 17 11 19  6  7  9 17 21  6  7 10]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.1025022641218956\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.1025022641218956 0.19431 2.32712 0.9073\n",
            "Processing input: [15 13 19 13 12 14 19 10  9  8 19  6 10 11  9  1 18  6  7  5]\n",
            "[15 13 19 13 12 14 19 10  9  8 19  6 10 11  9  1 18  6  7  5] <class 'numpy.ndarray'>\n",
            "MRED:  [15 13 19 13 12 14 19 10  9  8 19  6 10 11  9  1 18  6  7  5]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  2.7195411476488665\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "2.7195411476488665 0.10644 2.1380399999999997 0.11349999999999999\n",
            "Processing input: [ 7 19 14 15  8  2 14  7  6  6  9 13  2 17 15  6 19 14 12 12]\n",
            "[ 7 19 14 15  8  2 14  7  6  6  9 13  2 17 15  6 19 14 12 12] <class 'numpy.ndarray'>\n",
            "MRED:  [ 7 19 14 15  8  2 14  7  6  6  9 13  2 17 15  6 19 14 12 12]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.11284697925755877\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.11284697925755877 0.19431 2.28036 0.8268000000000001\n",
            "Processing input: [ 9  4  5 19  0  7  9  0  4 20 18 13  1 13  5 12 13  3 12 21]\n",
            "[ 9  4  5 19  0  7  9  0  4 20 18 13  1 13  5 12 13  3 12 21] <class 'numpy.ndarray'>\n",
            "MRED:  [ 9  4  5 19  0  7  9  0  4 20 18 13  1 13  5 12 13  3 12 21]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.44005088361343764\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.44005088361343764 0.1665 4.0895600000000005 0.6231\n",
            "Processing input: [20  7 15 20  2 20  4  2 11 12 12  5 21 20 12 11  1  1 10 16]\n",
            "[20  7 15 20  2 20  4  2 11 12 12  5 21 20 12 11  1  1 10 16] <class 'numpy.ndarray'>\n",
            "MRED:  [20  7 15 20  2 20  4  2 11 12 12  5 21 20 12 11  1  1 10 16]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.18146072622263498\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.18146072622263498 0.28093 3.56092 0.8741\n",
            "Processing input: [13 18  0  2 14  8 16 13 17  1  6 11 17 17  6  9  8  7 19  1]\n",
            "[13 18  0  2 14  8 16 13 17  1  6 11 17 17  6  9  8  7 19  1] <class 'numpy.ndarray'>\n",
            "MRED:  [13 18  0  2 14  8 16 13 17  1  6 11 17 17  6  9  8  7 19  1]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.1594771343127595\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.1594771343127595 0.18963000000000002 2.70784 0.8437\n",
            "Processing input: [11 14  7 16 19  9  4 16 14 11  2 20  6  5 21 16 16 21 20  1]\n",
            "[11 14  7 16 19  9  4 16 14 11  2 20  6  5 21 16 16 21 20  1] <class 'numpy.ndarray'>\n",
            "MRED:  [11 14  7 16 19  9  4 16 14 11  2 20  6  5 21 16 16 21 20  1]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.3591096262262783\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.3591096262262783 0.19431 2.96252 0.5735\n",
            "Processing input: [ 2  6  4  7  4 16 11 10 16 21  7 14 16 11  5 13 10 15  5  3]\n",
            "[ 2  6  4  7  4 16 11 10 16 21  7 14 16 11  5 13 10 15  5  3] <class 'numpy.ndarray'>\n",
            "MRED:  [ 2  6  4  7  4 16 11 10 16 21  7 14 16 11  5 13 10 15  5  3]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  1.3709387243878743\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "1.3709387243878743 0.19431 4.17556 0.1139\n",
            "Processing input: [12  9  1 12  9 16 14 16  8 21  3 10  5 14  8 11 10  9 19 18]\n",
            "[12  9  1 12  9 16 14 16  8 21  3 10  5 14  8 11 10  9 19 18] <class 'numpy.ndarray'>\n",
            "MRED:  [12  9  1 12  9 16 14 16  8 21  3 10  5 14  8 11 10  9 19 18]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  2.3573428514824903\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "2.3573428514824903 0.16942000000000002 2.6570200000000006 0.11349999999999999\n",
            "Processing input: [ 6  3 13 14 13 17  1 17 16  9 15  6 18 10 19 21  6 13 10 13]\n",
            "[ 6  3 13 14 13 17  1 17 16  9 15  6 18 10 19 21  6 13 10 13] <class 'numpy.ndarray'>\n",
            "MRED:  [ 6  3 13 14 13 17  1 17 16  9 15  6 18 10 19 21  6 13 10 13]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.20848134651587483\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.20848134651587483 0.1395 2.30828 0.9022\n",
            "Processing input: [12 19 12  3 16 11  5  7 20 14  3 10 18 10 13  8  3  2  3 13]\n",
            "[12 19 12  3 16 11  5  7 20 14  3 10 18 10 13  8  3  2  3 13] <class 'numpy.ndarray'>\n",
            "MRED:  [12 19 12  3 16 11  5  7 20 14  3 10 18 10 13  8  3  2  3 13]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.20869342153215684\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.20869342153215684 0.25393 3.73416 0.8654999999999999\n",
            "Processing input: [ 0 11 14 12  4  1 12 10 10 20 16 20  8 12  7 10 15 15  4 15]\n",
            "[ 0 11 14 12  4  1 12 10 10 20 16 20  8 12  7 10 15 15  4 15] <class 'numpy.ndarray'>\n",
            "MRED:  [ 0 11 14 12  4  1 12 10 10 20 16 20  8 12  7 10 15 15  4 15]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.1118344979237789\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.1118344979237789 0.1665 3.0578399999999997 0.9098999999999999\n",
            "Processing input: [20  1 21 11 15  3 11  1  3 16  5 17  8  2 11  6 13  3 14 15]\n",
            "[20  1 21 11 15  3 11  1  3 16  5 17  8  2 11  6 13  3 14 15] <class 'numpy.ndarray'>\n",
            "MRED:  [20  1 21 11 15  3 11  1  3 16  5 17  8  2 11  6 13  3 14 15]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.18075439325623482\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.18075439325623482 0.19431 4.1531400000000005 0.8835\n",
            "Processing input: [ 7 17 14 15  7  2 14  7  6  6  9  6  2 17 18 18 21 14 12  6]\n",
            "[ 7 17 14 15  7  2 14  7  6  6  9  6  2 17 18 18 21 14 12  6] <class 'numpy.ndarray'>\n",
            "MRED:  [ 7 17 14 15  7  2 14  7  6  6  9  6  2 17 18 18 21 14 12  6]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.35740923669632974\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.35740923669632974 0.19431 2.2382799999999996 0.7775\n",
            "Processing input: [13 19 11 17  9 19  8 19  6 15 11  0  0  3 19  7 15  1  4  2]\n",
            "[13 19 11 17  9 19  8 19  6 15 11  0  0  3 19  7 15  1  4  2] <class 'numpy.ndarray'>\n",
            "MRED:  [13 19 11 17  9 19  8 19  6 15 11  0  0  3 19  7 15  1  4  2]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.07379557558699636\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.07379557558699636 0.15437 3.163 0.9102\n",
            "Processing input: [12  5 10  4 11 13 10 13 11  4 20  4 10  9 17 21  9  7  1  9]\n",
            "[12  5 10  4 11 13 10 13 11  4 20  4 10  9 17 21  9  7  1  9] <class 'numpy.ndarray'>\n",
            "MRED:  [12  5 10  4 11 13 10 13 11  4 20  4 10  9 17 21  9  7  1  9]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.09445358740680714\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.09445358740680714 0.1665 3.86566 0.9063\n",
            "Processing input: [ 1  6 11  7  2 16 15 11  8 19  4 16 14 13  8 13 10  9  1  3]\n",
            "[ 1  6 11  7  2 16 15 11  8 19  4 16 14 13  8 13 10  9  1  3] <class 'numpy.ndarray'>\n",
            "MRED:  [ 1  6 11  7  2 16 15 11  8 19  4 16 14 13  8 13 10  9  1  3]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.08580138071505768\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.08580138071505768 0.19431 3.58774 0.9031\n",
            "Processing input: [ 7  5  7 20 11 18  0  6  3 13  2 15 21  1  4  4 19 10 17 17]\n",
            "[ 7  5  7 20 11 18  0  6  3 13  2 15 21  1  4  4 19 10 17 17] <class 'numpy.ndarray'>\n",
            "MRED:  [ 7  5  7 20 11 18  0  6  3 13  2 15 21  1  4  4 19 10 17 17]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.13323531435265032\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.13323531435265032 0.25312 3.9708200000000002 0.8487\n",
            "Processing input: [12  8 16 12 18 20 20  5 15  9 14 14 19  2 20  4  4 18 11 10]\n",
            "[12  8 16 12 18 20 20  5 15  9 14 14 19  2 20  4  4 18 11 10] <class 'numpy.ndarray'>\n",
            "MRED:  [12  8 16 12 18 20 20  5 15  9 14 14 19  2 20  4  4 18 11 10]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.6699142465740969\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.6699142465740969 0.28093 2.9432 0.29359999999999997\n",
            "Processing input: [10  7  8  8  5  5 21 21 17  7  1  7  7 11  9  9  8 15 15  2]\n",
            "[10  7  8  8  5  5 21 21 17  7  1  7  7 11  9  9  8 15 15  2] <class 'numpy.ndarray'>\n",
            "MRED:  [10  7  8  8  5  5 21 21 17  7  1  7  7 11  9  9  8 15 15  2]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.10756121667039241\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.10756121667039241 0.15437 3.0860000000000007 0.9109\n",
            "Processing input: [18 17  0  3 14  8 16 13 17  0  6 11 17  6  6  2 16  7 20  6]\n",
            "[18 17  0  3 14  8 16 13 17  0  6 11 17  6  6  2 16  7 20  6] <class 'numpy.ndarray'>\n",
            "MRED:  [18 17  0  3 14  8 16 13 17  0  6 11 17  6  6  2 16  7 20  6]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.16029414213404125\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.16029414213404125 0.19431 2.5178399999999996 0.7879999999999999\n",
            "Processing input: [ 6  3 13 14 13 20  9  9 15  3 15 13 18 10 19  5  6 12  9 13]\n",
            "[ 6  3 13 14 13 20  9  9 15  3 15 13 18 10 19  5  6 12  9 13] <class 'numpy.ndarray'>\n",
            "MRED:  [ 6  3 13 14 13 20  9  9 15  3 15 13 18 10 19  5  6 12  9 13]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.4298716155925653\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.4298716155925653 0.1395 2.65794 0.7581\n",
            "Processing input: [20  6  7  5 20  7 12 21  7 12 17 10 20  5 10 15  2 12  7  2]\n",
            "[20  6  7  5 20  7 12 21  7 12 17 10 20  5 10 15  2 12  7  2] <class 'numpy.ndarray'>\n",
            "MRED:  [20  6  7  5 20  7 12 21  7 12 17 10 20  5 10 15  2 12  7  2]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.3053209628526939\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.3053209628526939 0.19431 2.8446 0.8146\n",
            "Processing input: [12  5 20  2 11  7 10  3 11  4 17  4 15  6  7 20  9  7  1  5]\n",
            "[12  5 20  2 11  7 10  3 11  4 17  4 15  6  7 20  9  7  1  5] <class 'numpy.ndarray'>\n",
            "MRED:  [12  5 20  2 11  7 10  3 11  4 17  4 15  6  7 20  9  7  1  5]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  2.2206319741178073\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "2.2206319741178073 0.22612 4.36406 0.11349999999999999\n",
            "Processing input: [ 5 17 16 11 10 13  7 15 13 17 10  7 11  2  0 16 19 16  0  7]\n",
            "[ 5 17 16 11 10 13  7 15 13 17 10  7 11  2  0 16 19 16  0  7] <class 'numpy.ndarray'>\n",
            "MRED:  [ 5 17 16 11 10 13  7 15 13 17 10  7 11  2  0 16 19 16  0  7]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.12332409284283778\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.12332409284283778 0.19431 2.579 0.8201999999999999\n",
            "Processing input: [ 1  2  7  9 21 18 17 14  8 19  3  9 19 18  4  2 10 10 13 17]\n",
            "[ 1  2  7  9 21 18 17 14  8 19  3  9 19 18  4  2 10 10 13 17] <class 'numpy.ndarray'>\n",
            "MRED:  [ 1  2  7  9 21 18 17 14  8 19  3  9 19 18  4  2 10 10 13 17]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.36386236966297447\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.36386236966297447 0.28093 3.26218 0.8238\n",
            "Processing input: [10 11  9  0 19  5 13 13 12 10 12  3  7  8 17  2 12 11 18 19]\n",
            "[10 11  9  0 19  5 13 13 12 10 12  3  7  8 17  2 12 11 18 19] <class 'numpy.ndarray'>\n",
            "MRED:  [10 11  9  0 19  5 13 13 12 10 12  3  7  8 17  2 12 11 18 19]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  1.1780347059200724\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "1.1780347059200724 0.19431 2.6866800000000004 0.114\n",
            "Processing input: [ 1  9  3 16  7  0 20  9 11  3 10 12 18 16 18 18 18 21  9 11]\n",
            "[ 1  9  3 16  7  0 20  9 11  3 10 12 18 16 18 18 18 21  9 11] <class 'numpy.ndarray'>\n",
            "MRED:  [ 1  9  3 16  7  0 20  9 11  3 10 12 18 16 18 18 18 21  9 11]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  1.0326585055912227\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "1.0326585055912227 0.1395 2.5022800000000007 0.2335\n",
            "Processing input: [16 13 13 16 10  4  3 18  2 17  2  0 11  9  7 14 15  8 13 20]\n",
            "[16 13 13 16 10  4  3 18  2 17  2  0 11  9  7 14 15  8 13 20] <class 'numpy.ndarray'>\n",
            "MRED:  [16 13 13 16 10  4  3 18  2 17  2  0 11  9  7 14 15  8 13 20]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.2429890075516321\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.2429890075516321 0.22612 3.246580000000001 0.536\n",
            "Processing input: [ 4  2 17 14  1 18 12  5 13  2 21 10 14 17  1  4 14 10 16  9]\n",
            "[ 4  2 17 14  1 18 12  5 13  2 21 10 14 17  1  4 14 10 16  9] <class 'numpy.ndarray'>\n",
            "MRED:  [ 4  2 17 14  1 18 12  5 13  2 21 10 14 17  1  4 14 10 16  9]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.15036320410552817\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.15036320410552817 0.28093 4.33314 0.8771\n",
            "Processing input: [ 8  7  7  8  5  5 21 21 17  5  1  5 16  3 14  2  8 15 15  7]\n",
            "[ 8  7  7  8  5  5 21 21 17  5  1  5 16  3 14  2  8 15 15  7] <class 'numpy.ndarray'>\n",
            "MRED:  [ 8  7  7  8  5  5 21 21 17  5  1  5 16  3 14  2  8 15 15  7]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.28326323311084695\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.28326323311084695 0.25393 4.1622200000000005 0.8432999999999999\n",
            "Processing input: [17 11 16 18 15 17  7  2  8  5 16 16 14 11 19  0 12 16  3  5]\n",
            "[17 11 16 18 15 17  7  2  8  5 16 16 14 11 19  0 12 16  3  5] <class 'numpy.ndarray'>\n",
            "MRED:  [17 11 16 18 15 17  7  2  8  5 16 16 14 11 19  0 12 16  3  5]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  2.2791866791158295\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "2.2791866791158295 0.22423 3.1265799999999992 0.11349999999999999\n",
            "Processing input: [ 0  3 14 13  5  1  8  6 10  4  8 19  8 13  7 10 15 20  2 15]\n",
            "[ 0  3 14 13  5  1  8  6 10  4  8 19  8 13  7 10 15 20  2 15] <class 'numpy.ndarray'>\n",
            "MRED:  [ 0  3 14 13  5  1  8  6 10  4  8 19  8 13  7 10 15 20  2 15]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.1634958390976811\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.1634958390976811 0.1665 3.6053200000000007 0.8408\n",
            "Processing input: [ 1  6 11  7  3 10 15 11  0 19  4 16 15 13  8 13 19  9  1  3]\n",
            "[ 1  6 11  7  3 10 15 11  0 19  4 16 15 13  8 13 19  9  1  3] <class 'numpy.ndarray'>\n",
            "MRED:  [ 1  6 11  7  3 10 15 11  0 19  4 16 15 13  8 13 19  9  1  3]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.11535992535658275\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.11535992535658275 0.1665 3.645320000000001 0.8671\n",
            "Processing input: [18 12  9 11  0 20  9 16  2 19  8 12  6  9 16  7  7 12  9 14]\n",
            "[18 12  9 11  0 20  9 16  2 19  8 12  6  9 16  7  7 12  9 14] <class 'numpy.ndarray'>\n",
            "MRED:  [18 12  9 11  0 20  9 16  2 19  8 12  6  9 16  7  7 12  9 14]\n",
            "Code written to temp.py\n",
            "MRED_sampled:  0.09098003998725443\n",
            "Checking for GPU availability...\n",
            "Using device: cuda\n",
            "Defining the custom CNN model...\n",
            "0.09098003998725443 0.15447 1.959199999999999 0.9127\n",
            "     2 |      100 |      6 |  0.2170910660 |         ideal\n",
            "Pareto-optimal solutions:\n",
            "Solution 1: [ 5.72100192  8.29022582  9.08831137  4.83306786 17.489132    8.58785261\n",
            " 18.37976713 15.27314806 17.70141954 13.34589184 14.48669157  7.70755029\n",
            " 13.20285702 15.38642723 17.79027612  6.28813659  6.14463938 19.51630287\n",
            "  3.77134247  0.20678749]\n",
            "Power: 0.12656, Accuracy: 0.659\n",
            "Solution 2: [13.78920265 18.84566912 11.10312387 17.19945295 20.63303117 18.98102517\n",
            "  8.07014461 19.3132812   5.8599908  15.30906757 10.92234121  0.2762272\n",
            "  0.27536258  3.30360325 18.61677896  6.86839579 14.65744459  0.22368684\n",
            "  3.91982854  2.31272322]\n",
            "Power: 0.15437, Accuracy: 0.9116\n",
            "Solution 3: [ 6.46157172  3.37378601 13.38779051 14.0669425  13.41832449 19.60508915\n",
            "  9.39934688 16.86675609 14.81326206  8.62560015 14.73812644 12.47285984\n",
            "  5.98332697  9.88091621 19.36257298 20.66507821  5.50138274 12.0900159\n",
            "  9.37937682 12.70984537]\n",
            "Power: 0.1395, Accuracy: 0.9087000000000001\n",
            "Solution 4: [ 9.01840384  0.70407239  3.86798506 20.20566583 16.90811589  6.22625484\n",
            " 17.9445559   3.39386961 18.71636755  2.96470423 16.21517029  1.28025464\n",
            "  8.43118963  0.14257871 17.21629264 13.27185943  3.39728125 14.74825368\n",
            " 16.30129712  0.67234335]\n",
            "Power: 0.19912, Accuracy: 0.9155\n",
            "Solution 5: [15. 13. 19. 13. 12. 14. 19. 10.  9.  8. 19.  6. 10. 11.  9.  1. 18.  6.\n",
            "  7.  5.]\n",
            "Power: 0.10644, Accuracy: 0.11349999999999999\n",
            "Solution 6: [18. 12.  9. 11.  0. 20.  9. 16.  2. 19.  8. 12.  6.  9. 16.  7.  7. 12.\n",
            "  9. 14.]\n",
            "Power: 0.15447, Accuracy: 0.9127\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.operators.sampling.lhs import LHS\n",
        "from pymoo.optimize import minimize\n",
        "from pymoo.visualization.scatter import Scatter\n",
        "\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.operators.sampling.lhs import LHS\n",
        "from pymoo.operators.crossover.sbx import SimulatedBinaryCrossover\n",
        "from pymoo.operators.mutation.pm import PM\n",
        "from pymoo.operators.repair.rounding import RoundingRepair\n",
        "\n",
        "# CSV file for logging inputs and outputs\n",
        "log_file = \"optimization_log_lat_acc.csv\"\n",
        "\n",
        "# Write the header to the log file\n",
        "with open(log_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Input_Config\"] + [\"MRED\", \"Latency\", \"Power\", \"Accuracy\"])\n",
        "\n",
        "# Define the multi-objective problem\n",
        "class MultiObjectiveOptimization(Problem):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            n_var=20,  # Number of decision variables\n",
        "            n_obj=2,   # Number of objectives (Power, Accuracy)\n",
        "            # n_constr=0,  # No constraints\n",
        "            xl=np.zeros(20),  # Lower bounds for decision variables\n",
        "            xu=np.full(20, 21)  # Upper bounds for decision variables\n",
        "        )\n",
        "\n",
        "    def _evaluate(self, x, out, *args, **kwargs):\n",
        "        results = []\n",
        "        rows_to_log = []\n",
        "        for xi in x:\n",
        "            # Ensure xi is an integer (in case rounding wasn't applied elsewhere)\n",
        "            xi = np.round(xi).astype(int)\n",
        "            print(f\"Processing input: {xi}\")  # Debugging\n",
        "            error, latency, power, accuracy = multi_objective_function(xi)\n",
        "            rows_to_log.append([xi.tolist(), error, latency, power, accuracy])\n",
        "            results.append([latency, -accuracy])  # Accuracy negated for minimization\n",
        "        with open(log_file, \"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerows(rows_to_log)\n",
        "        out[\"F\"] = np.array(results)\n",
        "\n",
        "# Define the NSGA-II algorithm\n",
        "algorithm = NSGA2(\n",
        "    pop_size=50,  # Larger population size for large search space\n",
        "    sampling=LHS(),  # Latin Hypercube Sampling for better diversity\n",
        "    crossover=SimulatedBinaryCrossover(prob=0.9, eta=15),  # SBX with crossover probability\n",
        "    mutation=PM(prob=0.2, eta=3.0, vtype=float, repair=RoundingRepair()),  # Polynomial mutation\n",
        ")\n",
        "\n",
        "# Instantiate the problem\n",
        "problem = MultiObjectiveOptimization()\n",
        "\n",
        "# Run the optimization\n",
        "res = minimize(\n",
        "    problem,\n",
        "    algorithm,\n",
        "    termination=(\"n_gen\", 2),  # Increase generations for better convergence\n",
        "    seed=42,\n",
        "    save_history=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Visualize the Pareto front\n",
        "plot = Scatter(title=\"Pareto Front (Lat vs. Accuracy)\")\n",
        "plot.add(res.F, facecolor=\"red\", edgecolor=\"k\")\n",
        "plot.save(\"pareto_front_lat_acc.png\")\n",
        "\n",
        "# Extract Pareto-optimal solutions\n",
        "pareto_solutions = res.X\n",
        "pareto_objectives = res.F\n",
        "\n",
        "print(\"Pareto-optimal solutions:\")\n",
        "for i, (solution, objectives) in enumerate(zip(pareto_solutions, pareto_objectives)):\n",
        "    print(f\"Solution {i + 1}: {solution}\")\n",
        "    print(f\"Power: {objectives[0]}, Accuracy: {-objectives[1]}\")  # Accuracy is negated back\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "028EEUoUN4r9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}